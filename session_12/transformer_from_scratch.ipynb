{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFERENCE\n",
    "1. http://jalammar.github.io/illustrated-transformer/\n",
    "2. https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb\n",
    "3. Good lectures on attention mechanism and positional encoding https://www.youtube.com/watch?v=gJ9kaJsE78k\n",
    "4. https://www.youtube.com/watch?v=-9vVhYEXeyQ\n",
    "    \n",
    "# Todo\n",
    "- [ ] check why are we using src_mask in the 2nd multihead attn of decoder\n",
    "- [ ] why are we not using positional encoding using sine and cosine curve "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:38:07.413688Z",
     "start_time": "2021-02-25T03:38:07.409880Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T13:30:17.607357Z",
     "start_time": "2021-02-17T13:30:17.602114Z"
    }
   },
   "source": [
    "# GLOBAL SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:38:07.867550Z",
     "start_time": "2021-02-25T03:38:07.864876Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:38:09.511598Z",
     "start_time": "2021-02-25T03:38:08.318753Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:38:09.515481Z",
     "start_time": "2021-02-25T03:38:09.512852Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:38:09.521630Z",
     "start_time": "2021-02-25T03:38:09.516985Z"
    }
   },
   "outputs": [],
   "source": [
    "SRC = Field(tokenize=tokenize_de,\n",
    "            init_token='<sos>',\n",
    "            eos_token='<eos>',\n",
    "            lower=True,\n",
    "            batch_first=True)\n",
    "\n",
    "TRG = Field(tokenize=tokenize_en,\n",
    "            init_token='<sos>',\n",
    "            eos_token='<eos>',\n",
    "            lower=True,\n",
    "            batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:38:12.618467Z",
     "start_time": "2021-02-25T03:38:09.522626Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:38:12.751266Z",
     "start_time": "2021-02-25T03:38:12.619329Z"
    }
   },
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:38:12.754171Z",
     "start_time": "2021-02-25T03:38:12.752343Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:38:12.776120Z",
     "start_time": "2021-02-25T03:38:12.755300Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING THE MODEL\n",
    "We will do German to English translation where input/source sentence (in German)is passes through the encoder which gives a context vector. Using this context vector we will be decoding the German src sentence to English "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "![](https://charon.me/img/15871364052016.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:38:26.355073Z",
     "start_time": "2021-02-25T03:38:26.336865Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Encoder arguments\n",
    "        input_dim --> [src_len,batch_size]\n",
    "        hid_dim --> convert each input word to embedding\n",
    "        n_layers --> number of transformer layers\n",
    "        n_heads --> number of heads for multi head attention\n",
    "        pf_dim --> hidden layer upscaling in pointwise feedforward layer\n",
    "        dropout --> dropout value \n",
    "        device --> gpu or cpu\n",
    "        max_length --> max langth of the sentence, used in positional encoding\n",
    "        \n",
    "    Encoder outputs\n",
    "        src after changing it through the self attention and pointwise feed forward layer\n",
    "        \n",
    "    Encoder description\n",
    "        - combines source and positional embedding\n",
    "        - passes it through self attention layer and calculates attention\n",
    "        - passes attention output to pointwise feed forward layer \n",
    "        - returns the src after passing it to the aforementioned layers\n",
    "'''\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 hid_dim,\n",
    "                 n_layers,\n",
    "                 n_heads,\n",
    "                 pf_dim,\n",
    "                 dropout,\n",
    "                 device,\n",
    "                 max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length,hid_dim)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # src_mask is simply the same shape as the source and is 1 where <pad> is not there else 0\n",
    "        # src batch_size, src_len\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "\n",
    "        # pos batch_size, src_len\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size,\n",
    "                                                           1).to(device)\n",
    "        # src batch_size, src_len , hid_dim\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) +\n",
    "                           self.pos_embedding(pos))\n",
    "\n",
    "        # pass it to EncoderLayer and call it n_layers time\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:38:27.375077Z",
     "start_time": "2021-02-25T03:38:27.360563Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    EncoderLayer arguments\n",
    "    \n",
    "        hid_dim --> convert each input word to embedding\n",
    "        n_heads --> number of heads for multi head attention\n",
    "        pf_dim --> hidden layer upscaling in pointwise feedforward layer\n",
    "        dropout --> dropout value \n",
    "        device --> gpu or cpu\n",
    "        \n",
    "    EncoderLayer outputs\n",
    "        src after changing it through the self attention and pointwise feed forward layer\n",
    "        \n",
    "    EncoderLayer description\n",
    "        -  First It takes the src coming from encoder and passes it through attention mechanism.\n",
    "        -  Then applies the layer norm and feed forward operations on the transformed source\n",
    "'''\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.pointwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # src = [batch_size,src_len,hid_dim]\n",
    "        # src_mask = [batch_size,1,1,hid_dim] ## todo check this\n",
    "\n",
    "        # self attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src = self.self_attn_layer_norm(self.dropout(_src) + src)\n",
    "        _src = self.pointwise_feedforward(src)\n",
    "\n",
    "        #dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        #src = [batch size, src len, hid dim]\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHeadAttention Layer\n",
    "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/transformer-attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:38:28.686530Z",
     "start_time": "2021-02-25T03:38:28.663445Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    MultiHeadAttentionLayer arguments\n",
    "    \n",
    "        hid_dim --> convert each input word to embedding\n",
    "        n_heads --> number of heads for multi head attention\n",
    "        dropout --> dropout value \n",
    "        device --> gpu or cpu\n",
    "        \n",
    "    MultiHeadAttentionLayer outputs\n",
    "        src after changing it through the self attention mechanism and the attention mask\n",
    "        \n",
    "    MultiHeadAttentionLayer description\n",
    "        -  takes the query key and value matrices\n",
    "        -  computes energy by multiplying query and key matrices\n",
    "        -  apply mask on the energy vector\n",
    "        -  while processing the encoder src mask is the mask where the unnecessary pad tokens are removed\n",
    "        -  while processing the decoder trg mask is the mask where future information is masked so that\n",
    "        -  decoder can only see previous and current word while decoding next word\n",
    "        -  the energy matrix is then multiplied by the value matrix to get the final transformed source matrix\n",
    "        -  we also return the attention mask after applying a softmax on the energy vector \n",
    "        -  this could later be used to visually test the attention on words\n",
    "'''\n",
    "\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dims = hid_dim // n_heads\n",
    "\n",
    "        # explain why use 3 different layer , they are sorting the embedding data so that we can divide it later\n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        # hyper params\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dims])).to(device)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # query = [batch_size,query_len,hid_dim]\n",
    "        # key = [batch_size,key_len,hid_dim]\n",
    "        # value = [batch_size,value_len,hid_dim]\n",
    "\n",
    "        ## SORTING TO KEEP SIMILAR THINGS TOGETHER\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_v(value)\n",
    "        V = self.fc_k(key)\n",
    "\n",
    "        ## DIVIDE THE\n",
    "        # if we have 16 heads with 1024 hid dim then head_dims is 1024 // 16 = 64\n",
    "        Q = Q.view(batch_size, -1, self.n_heads,\n",
    "                   self.head_dims).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads,\n",
    "                   self.head_dims).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads,\n",
    "                   self.head_dims).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Q.K^T\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            ## todo check out this operation [batch_size,n_heads,query_len,key_len] vs [batch_size,1,1,key_len]\n",
    "            energy = energy.masked_fill(mask == 0, -1e10) \n",
    "        # attention = [batch_size,n_heads,query_len,key_len]\n",
    "        attention = self.dropout(torch.softmax(energy, dim=-1))\n",
    "\n",
    "        # x [batch_size,n_heads,query_len,head_dims]\n",
    "        x = torch.matmul(attention, V)\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        # wkt self.head_dims = hid_dim // n_heads, and wakt x dim [batch_size,n_heads,query_len,head_dims]\n",
    "        # so we get hid_dim, and whatever is left is query_len\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "\n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:38:34.192350Z",
     "start_time": "2021-02-25T03:38:34.177498Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    PositionwiseFeedforwardLayer arguments\n",
    "    \n",
    "        hid_dim --> convert each input word to embedding\n",
    "        pf_dim --> hidden layer upscaling in pointwise feedforward layer\n",
    "        dropout --> dropout value \n",
    "        \n",
    "    PositionwiseFeedforwardLayer outputs\n",
    "        src after changing it through couple of linear layers and relu activation\n",
    "        \n",
    "'''\n",
    "\n",
    "\n",
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #x = [batch size, seq len, hid dim]\n",
    "\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "\n",
    "        #x = [batch size, seq len, pf dim]\n",
    "\n",
    "        x = self.fc_2(x)\n",
    "\n",
    "        #x = [batch size, seq len, hid dim]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/transformer-decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:38:38.405797Z",
     "start_time": "2021-02-25T03:38:38.387194Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Decoder arguments\n",
    "        output_dim --> [teg_len,batch_size]\n",
    "        hid_dim --> convert each input word to embedding\n",
    "        n_layers --> number of transformer layers\n",
    "        n_heads --> number of heads for multi head attention\n",
    "        pf_dim --> \n",
    "        dropout --> dropout value \n",
    "        device --> gpu or cpu\n",
    "        max_length --> max langth of the sentence, used in positional encoding\n",
    "        \n",
    "    Decoder outputs\n",
    "        trg after changing it through the layers shown in the figure above\n",
    "        \n",
    "    Decoder description\n",
    "        - combines trg and postional embedding\n",
    "        - passes it through the decoder layers as shown in the figure \n",
    "        - the final output is passed through linear layer to match output vocab dimension\n",
    "        - we would later do softmax on this output to find the best word prediction\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_dim,\n",
    "                 hid_dim,\n",
    "                 n_layers,\n",
    "                 n_heads,\n",
    "                 pf_dim,\n",
    "                 dropout,\n",
    "                 device,\n",
    "                 max_length=100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        # initital layers\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "\n",
    "        # processing layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "        # last layers\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "\n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        #pos = [batch size, trg len]\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size,\n",
    "                                                           1).to(self.device)\n",
    "\n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) +\n",
    "                           self.pos_embedding(pos))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        #output = [batch size, trg len, output dim]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/transformer-decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:38:40.078413Z",
     "start_time": "2021-02-25T03:38:40.057615Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    DecoderLayer arguments\n",
    "    \n",
    "        hid_dim --> convert each input word to embedding\n",
    "        n_heads --> number of heads for multi head attention\n",
    "        pf_dim --> hidden layer upscaling in pointwise feedforward layer\n",
    "        dropout --> dropout value \n",
    "        device --> gpu or cpu\n",
    "        \n",
    "    DecoderLayer outputs\n",
    "        trg after changing it through the self attention and pointwise feed forward layers\n",
    "        \n",
    "    DecoderLayer description\n",
    "        - The decoder layer is almost as same as encoder layer with thr following changes\n",
    "        - decoder layer uses 2 attention layers\n",
    "        - first trg goes through self attention layer and attention is calculated, here trg is masked using trg_mask\n",
    "        - attention output is then passed through add and layernorm block\n",
    "        - again we calculate attention using encoder key value and decoder query\n",
    "        - this 2nd attention output is then passed through add and layernorm block\n",
    "'''\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads,\n",
    "                                                      dropout, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(\n",
    "            hid_dim, n_heads, dropout, device)\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(\n",
    "            hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        # trg = [batch_size, trg_len, hid_dim]\n",
    "        # enc_src = [batch_size, src_len, hid_dim]\n",
    "        # trg_mask = [batch_size,1,trg_len,trg_len] ## todo check this dim\n",
    "        # src_mask = [batch_size,1,1,trg_len] ## because we will multiply this with energy vector in the attention block\n",
    "\n",
    "        #self attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "\n",
    "        #encoder attention\n",
    "        _trg, attention = self.encoder_attention(\n",
    "            trg, enc_src, enc_src, src_mask)  ## todo why src_mask not trg_mask\n",
    "        # query, key, value\n",
    "\n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        #positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "\n",
    "        #dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "\n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:38:40.945434Z",
     "start_time": "2021-02-25T03:38:40.928374Z"
    }
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,src_pad_idx,trg_pad_idx,device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self,src):\n",
    "        # trg = [batch_size,trg_len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        ## todo check this func\n",
    "        #trg = [batch size, trg len]\n",
    "    \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)    \n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        \n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        \n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        \n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:34:35.569785Z",
     "start_time": "2021-02-25T03:34:35.565937Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.arange(0,20).repeat(4,1).shape , torch.arange(0,20).unsqueeze(0).repeat(4,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:34:36.830133Z",
     "start_time": "2021-02-25T03:34:35.570993Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:34:36.844289Z",
     "start_time": "2021-02-25T03:34:36.831105Z"
    }
   },
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:34:36.850884Z",
     "start_time": "2021-02-25T03:34:36.845193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC.vocab.stoi[SRC.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:34:36.855851Z",
     "start_time": "2021-02-25T03:34:36.853366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 9,038,597 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:34:36.859778Z",
     "start_time": "2021-02-25T03:34:36.856985Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:34:36.864823Z",
     "start_time": "2021-02-25T03:34:36.860594Z"
    }
   },
   "outputs": [],
   "source": [
    "model.apply(initialize_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:34:36.868273Z",
     "start_time": "2021-02-25T03:34:36.865760Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:34:36.872372Z",
     "start_time": "2021-02-25T03:34:36.869170Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:34:36.876811Z",
     "start_time": "2021-02-25T03:34:36.873269Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:34:36.882044Z",
     "start_time": "2021-02-25T03:34:36.877794Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:34:36.886221Z",
     "start_time": "2021-02-25T03:34:36.882997Z"
    }
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T03:36:50.896200Z",
     "start_time": "2021-02-25T03:34:36.887156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 12s\n",
      "\tTrain Loss: 4.230 | Train PPL:  68.700\n",
      "\t Val. Loss: 2.995 |  Val. PPL:  19.991\n",
      "Epoch: 02 | Time: 0m 13s\n",
      "\tTrain Loss: 2.788 | Train PPL:  16.246\n",
      "\t Val. Loss: 2.275 |  Val. PPL:   9.729\n",
      "Epoch: 03 | Time: 0m 13s\n",
      "\tTrain Loss: 2.220 | Train PPL:   9.205\n",
      "\t Val. Loss: 1.975 |  Val. PPL:   7.207\n",
      "Epoch: 04 | Time: 0m 13s\n",
      "\tTrain Loss: 1.877 | Train PPL:   6.532\n",
      "\t Val. Loss: 1.805 |  Val. PPL:   6.078\n",
      "Epoch: 05 | Time: 0m 13s\n",
      "\tTrain Loss: 1.636 | Train PPL:   5.132\n",
      "\t Val. Loss: 1.706 |  Val. PPL:   5.507\n",
      "Epoch: 06 | Time: 0m 13s\n",
      "\tTrain Loss: 1.446 | Train PPL:   4.247\n",
      "\t Val. Loss: 1.647 |  Val. PPL:   5.190\n",
      "Epoch: 07 | Time: 0m 13s\n",
      "\tTrain Loss: 1.296 | Train PPL:   3.654\n",
      "\t Val. Loss: 1.627 |  Val. PPL:   5.087\n",
      "Epoch: 08 | Time: 0m 12s\n",
      "\tTrain Loss: 1.171 | Train PPL:   3.225\n",
      "\t Val. Loss: 1.623 |  Val. PPL:   5.069\n",
      "Epoch: 09 | Time: 0m 13s\n",
      "\tTrain Loss: 1.062 | Train PPL:   2.892\n",
      "\t Val. Loss: 1.614 |  Val. PPL:   5.022\n",
      "Epoch: 10 | Time: 0m 13s\n",
      "\tTrain Loss: 0.967 | Train PPL:   2.631\n",
      "\t Val. Loss: 1.645 |  Val. PPL:   5.183\n"
     ]
    }
   ],
   "source": [
    "# to understand how the training and loss works check out the last part of the following blog\n",
    "# http://jalammar.github.io/illustrated-transformer/\n",
    "# we feed first trg <start> token to decoder and decoder predicts the next token\n",
    "# so while computing the loss we ignore <start> token from trg original and <end> from the target ??\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
