{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implimentation of the paper Convolutional Sequence to Sequence Learning\n",
    "## https://arxiv.org/abs/1705.03122\n",
    "\n",
    "### This code is a replimentation of the code https://github.com/bentrevett/pytorch-seq2seq/blob/master/5%20-%20Convolutional%20Sequence%20to%20Sequence%20Learning.ipynb \n",
    ". I have added(some borrowed) comments for my own reference. Please check out the original work for more details.\n",
    "\n",
    "## This notebook has the following parts \n",
    "    - Data processing steps for German to English translation\n",
    "    - Encoder and Decoder architecture with their implementation details\n",
    "    - Code for prediction(translate from German to English)\n",
    "    \n",
    "## TODO:\n",
    "- [x] Add description for Encoder and Decoder architecture and implementation\n",
    "- [x] Add description for prediction code\n",
    "- [ ] Add description for attention block inside the decoder\n",
    "- [ ] Add description for display_attention function  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T15:10:46.124832Z",
     "start_time": "2021-01-28T15:10:46.120747Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T15:10:46.596122Z",
     "start_time": "2021-01-28T15:10:46.587282Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T15:10:48.145360Z",
     "start_time": "2021-01-28T15:10:47.003958Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T15:10:48.148711Z",
     "start_time": "2021-01-28T15:10:48.146414Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T15:10:48.153957Z",
     "start_time": "2021-01-28T15:10:48.150214Z"
    }
   },
   "outputs": [],
   "source": [
    "SRC = Field(tokenize=tokenize_de,\n",
    "            init_token='<sos>',\n",
    "            eos_token='<eos>',\n",
    "            lower=True,\n",
    "            batch_first=True)\n",
    "\n",
    "TRG = Field(tokenize=tokenize_en,\n",
    "            init_token='<sos>',\n",
    "            eos_token='<eos>',\n",
    "            lower=True,\n",
    "            batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T15:10:51.062423Z",
     "start_time": "2021-01-28T15:10:48.155035Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T15:10:51.190044Z",
     "start_time": "2021-01-28T15:10:51.063589Z"
    }
   },
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T15:10:51.192696Z",
     "start_time": "2021-01-28T15:10:51.191014Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T15:10:51.213616Z",
     "start_time": "2021-01-28T15:10:51.193564Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder :\n",
    "    1. The encoder architecture uses CONV blocks to encode the tokes/words (instead of GRU/LSTM).\n",
    "    2. The encoder returns 2 outputs, combined output and conved output while the input is word token\n",
    "    3. Let's understand the encoder in 2 parts, the first part is the overall structure and the 2nd part is the convolution blocks used\n",
    "    4. We will be using the images shown below to describe the operations \n",
    "    \n",
    "### Encoder architecture:\n",
    "1. **token and positional embeddings :** We use 2 embedding layer to encode the token and the position of the token. Unlike LSTM/GRU convolution will be applied for the entire sentence. The concept of time-steps won't be applicable here(because conv runs on the entire input) But we need the positional data for inference purpose, so we encode this info using a emb layer.<br>\n",
    "\n",
    "2. **element wise sum :** we perform element wise sum for the pos_embedding and tok_embedding. This is called **embedding verctor**<br>\n",
    "\n",
    "3. **linear layer emb dim --> hid dim :** Linear layer to convert embedding verctor to conv input with size of hid_size. This is **conv input**<br>\n",
    "\n",
    "4. **conv block :** We perform convolution here. We use pytorch nn.ModuleList to perform the conv. More on conv will be discussed in the next section.<br>\n",
    "\n",
    "5. **linear layer hid dim --> emb dim:** Linear layer to convert hid dim to emb dim. This output is called **conved output**<br>\n",
    "\n",
    "6. **residual connection** : We perform element wise sum for the conved output and tok_embedding. This  could be thought of as a residual connection to pass information from the initial layer to the bottleneck layers<br>\n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-seq2seq/raw/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq1.png)<br>\n",
    "\n",
    "### Convolution Blocks:\n",
    "1. **conv input** : We pad the input seq. We do that to keep keep the input and output dim to be same\n",
    "2. **glu activation** : Check out torch documentation for GLU. One thing is GLU output is half the input size. We need to adjust conv output accordingly.\n",
    "3. **conv layers** : we send each layer output to the next conv layer as input. We do this for n_layers.<br>\n",
    "\n",
    "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T15:19:43.752229Z",
     "start_time": "2021-01-28T15:19:43.743023Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 emb_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 kernel_size, \n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size, \n",
    "                                              padding = (kernel_size - 1) // 2)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        #create position tensor\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [0, 1, 2, 3, ..., src len - 1]\n",
    "        \n",
    "        #pos = [batch size, src len]\n",
    "        \n",
    "        #embed tokens and positions\n",
    "        tok_embedded = self.tok_embedding(src)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        #tok_embedded = pos_embedded = [batch size, src len, emb dim]\n",
    "        \n",
    "        #combine embeddings by elementwise summing\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        \n",
    "        #embedded = [batch size, src len, emb dim]\n",
    "        \n",
    "        #pass embedded through linear layer to convert from emb dim to hid dim\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        \n",
    "        #conv_input = [batch size, src len, hid dim]\n",
    "        \n",
    "        #permute for convolutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1) \n",
    "        \n",
    "        #conv_input = [batch size, hid dim, src len]\n",
    "        \n",
    "        #begin convolutional blocks...\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            #pass through convolutional layer\n",
    "            conved = conv(self.dropout(conv_input))\n",
    "\n",
    "            #conved = [batch size, 2 * hid dim, src len]\n",
    "\n",
    "            #pass through GLU activation function\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "\n",
    "            #conved = [batch size, hid dim, src len]\n",
    "            \n",
    "            #apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "\n",
    "            #conved = [batch size, hid dim, src len]\n",
    "            \n",
    "            #set conv_input to conved for next loop iteration\n",
    "            conv_input = conved\n",
    "        \n",
    "        #...end convolutional blocks\n",
    "        \n",
    "        #permute and convert back to emb dim\n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        #conved = [batch size, src len, emb dim]\n",
    "        \n",
    "        #elementwise sum output (conved) and input (embedded) to be used for attention\n",
    "        combined = (conved + embedded) * self.scale\n",
    "        \n",
    "        #combined = [batch size, src len, emb dim]\n",
    "        \n",
    "        return conved, combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docoder :\n",
    "    1. The decoder architecture is almost same as encoder with slight changes as described below.\n",
    "    2. If we look at the decoder input we see that we are feeding the original target sentence(basically label). This might look bizarre. But once we discuss the conv blocks it will be cleared. \n",
    "    3. The conv input will be shifted(using padding) in such a way that each conv block will see only one input word.\n",
    "    4. Using this input along with the encoder features , the decoder will try to output the next word.\n",
    "    5. One thing to notice here is that we are feeding the embedding (after concatenating token+pos embedding) directly to the conv block along with other inputs. In the encoder this step was not done. So conv here has in total 4 inputs, embedding directly, 2 outputs of encoder and the emb_2_hid vector\n",
    "    4. We will be using the images shown below to describe the conv operations\n",
    "    \n",
    "    \n",
    "### Docoder architecture:\n",
    "1. **token and positional embeddings :** Similar to encoder we use 2 embedding layers to encode the token and the position of the token.<br>\n",
    "2. **element wise sum :** We perform element wise sum for the pos_embedding and tok_embedding. This is called **embedding verctor**<br>\n",
    "3. **linear layer emb dim --> hid dim :** Linear layer to convert embedding verctor to conv input with size of hid_size. This is called **conv input**<br>\n",
    "4. **conv block :** We perform convolution here. We use pytorch nn.ModuleList to perform the conv. More on conv will be discussed in the next section.<br>\n",
    "5. **linear layer hid dim --> emb dim:** Linear layer to convert hid dim to emb dim. This output is **conved output**<br>\n",
    "\n",
    "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq3.png)<br>\n",
    "\n",
    "### Convolution Blocks:\n",
    "    The convolution follows the same architecture as decoder.The changes in case of decoder are discussed below.<br>\n",
    "    \n",
    "1. **conv input** : The conv has 4 inputs namely the embedding vector, the hidden vector, the encoder outputs conved and the combined vector.\n",
    "\n",
    "2. **conv input padding** Similar to image, we pad the input seq. We will only pad twice at the start. This was done differently in the encoder where we padded evenly. As we are processing all of the targets simultaneously in parallel, and not sequentially, we need a method of only allowing the filters translating token $i$ to only look at tokens before word $i$. If they were allowed to look at token $i+1$ (the token they should be outputting), the model will simply learn to output the next word in the sequence by directly copying it, without actually learning how to translate.\n",
    "\n",
    "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T15:19:44.886182Z",
     "start_time": "2021-01-28T15:19:44.876861Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_dim, \n",
    "                 emb_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 kernel_size, \n",
    "                 dropout, \n",
    "                 trg_pad_idx, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "      \n",
    "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
    "        \n",
    "        #embedded = [batch size, trg len, emb dim]\n",
    "        #conved = [batch size, hid dim, trg len]\n",
    "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
    "        \n",
    "        #permute and convert back to emb dim\n",
    "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        #conved_emb = [batch size, trg len, emb dim]\n",
    "        \n",
    "        combined = (conved_emb + embedded) * self.scale\n",
    "        \n",
    "        #combined = [batch size, trg len, emb dim]\n",
    "                \n",
    "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n",
    "        \n",
    "        #energy = [batch size, trg len, src len]\n",
    "        \n",
    "        attention = F.softmax(energy, dim=2)\n",
    "        \n",
    "        #attention = [batch size, trg len, src len]\n",
    "            \n",
    "        attended_encoding = torch.matmul(attention, encoder_combined)\n",
    "        \n",
    "        #attended_encoding = [batch size, trg len, emd dim]\n",
    "        \n",
    "        #convert from emb dim -> hid dim\n",
    "        attended_encoding = self.attn_emb2hid(attended_encoding)\n",
    "        \n",
    "        #attended_encoding = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #apply residual connection\n",
    "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n",
    "        \n",
    "        #attended_combined = [batch size, hid dim, trg len]\n",
    "        \n",
    "        return attention, attended_combined\n",
    "        \n",
    "    def forward(self, trg, encoder_conved, encoder_combined):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
    "                \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "            \n",
    "        #create position tensor\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [batch size, trg len]\n",
    "        \n",
    "        #embed tokens and positions\n",
    "        tok_embedded = self.tok_embedding(trg)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        #tok_embedded = [batch size, trg len, emb dim]\n",
    "        #pos_embedded = [batch size, trg len, emb dim]\n",
    "        \n",
    "        #combine embeddings by elementwise summing\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        \n",
    "        #embedded = [batch size, trg len, emb dim]\n",
    "        \n",
    "        #pass embedded through linear layer to go through emb dim -> hid dim\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        \n",
    "        #conv_input = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #permute for convolutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1) \n",
    "        \n",
    "        #conv_input = [batch size, hid dim, trg len]\n",
    "        \n",
    "        batch_size = conv_input.shape[0]\n",
    "        hid_dim = conv_input.shape[1]\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            #apply dropout\n",
    "            conv_input = self.dropout(conv_input)\n",
    "        \n",
    "            #need to pad so decoder can't \"cheat\"\n",
    "            padding = torch.zeros(batch_size, \n",
    "                                  hid_dim, \n",
    "                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device) ## ??todo\n",
    "                \n",
    "            padded_conv_input = torch.cat((padding, conv_input), dim = 2)\n",
    "        \n",
    "            #padded_conv_input = [batch size, hid dim, trg len + kernel size - 1]\n",
    "        \n",
    "            #pass through convolutional layer\n",
    "            conved = conv(padded_conv_input)\n",
    "\n",
    "            #conved = [batch size, 2 * hid dim, trg len]\n",
    "            \n",
    "            #pass through GLU activation function\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "\n",
    "            #conved = [batch size, hid dim, trg len]\n",
    "            \n",
    "            #calculate attention\n",
    "            attention, conved = self.calculate_attention(embedded, \n",
    "                                                         conved, \n",
    "                                                         encoder_conved, \n",
    "                                                         encoder_combined)\n",
    "            \n",
    "            #attention = [batch size, trg len, src len]\n",
    "            \n",
    "            #apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "            \n",
    "            #conved = [batch size, hid dim, trg len]\n",
    "            \n",
    "            #set conv_input to conved for next loop iteration\n",
    "            conv_input = conved\n",
    "            \n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "         \n",
    "        #conved = [batch size, trg len, emb dim]\n",
    "            \n",
    "        output = self.fc_out(self.dropout(conved))\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "            \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T15:19:47.366994Z",
     "start_time": "2021-01-28T15:19:47.355414Z"
    }
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len - 1] (<eos> token sliced off the end)\n",
    "           \n",
    "        #calculate z^u (encoder_conved) and (z^u + e) (encoder_combined)\n",
    "        #encoder_conved is output from final encoder conv. block\n",
    "        #encoder_combined is encoder_conved plus (elementwise) src embedding plus \n",
    "        #  positional embeddings \n",
    "        encoder_conved, encoder_combined = self.encoder(src)\n",
    "            \n",
    "        #encoder_conved = [batch size, src len, emb dim]\n",
    "        #encoder_combined = [batch size, src len, emb dim]\n",
    "        \n",
    "        #calculate predictions of next words\n",
    "        #output is a batch of predictions for each word in the trg sentence\n",
    "        #attention a batch of attention scores across the src sentence for \n",
    "        #  each word in the trg sentence\n",
    "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n",
    "        \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #attention = [batch size, trg len - 1, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T15:19:52.850334Z",
     "start_time": "2021-01-28T15:19:50.044058Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\n",
    "ENC_LAYERS = 10 # number of conv. blocks in encoder\n",
    "DEC_LAYERS = 10 # number of conv. blocks in decoder\n",
    "ENC_KERNEL_SIZE = 3 # must be odd!\n",
    "DEC_KERNEL_SIZE = 3 # can be even or odd\n",
    "ENC_DROPOUT = 0.25\n",
    "DEC_DROPOUT = 0.25\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "    \n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, TRG_PAD_IDX, device)\n",
    "\n",
    "model = Seq2Seq(enc, dec).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T15:19:54.529971Z",
     "start_time": "2021-01-28T15:19:54.516271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 37,351,429 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T15:19:56.138262Z",
     "start_time": "2021-01-28T15:19:56.131401Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T15:20:01.033129Z",
     "start_time": "2021-01-28T15:20:01.019756Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "        \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "        \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T15:20:04.344190Z",
     "start_time": "2021-01-28T15:20:04.331747Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "        \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T15:20:05.353392Z",
     "start_time": "2021-01-28T15:20:05.348469Z"
    }
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T15:52:06.786337Z",
     "start_time": "2021-01-27T15:45:58.771415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 35s\n",
      "\tTrain Loss: 4.108421112472265 | Train PPL: 60.850565484034185\n",
      "\t Val. Loss: 3.021039992570877 |  Val. PPL: 20.51261356068697\n",
      "Epoch: 02 | Time: 0m 36s\n",
      "\tTrain Loss: 3.00443079398067 | Train PPL: 20.174729249779222\n",
      "\t Val. Loss: 2.371318280696869 |  Val. PPL: 10.711503749258434\n",
      "Epoch: 03 | Time: 0m 36s\n",
      "\tTrain Loss: 2.5889812261522605 | Train PPL: 13.31619849562757\n",
      "\t Val. Loss: 2.1378494054079056 |  Val. PPL: 8.481178425395226\n",
      "Epoch: 04 | Time: 0m 36s\n",
      "\tTrain Loss: 2.36228563291911 | Train PPL: 10.615186164996153\n",
      "\t Val. Loss: 2.0146346241235733 |  Val. PPL: 7.497987296973038\n",
      "Epoch: 05 | Time: 0m 37s\n",
      "\tTrain Loss: 2.2049008397803957 | Train PPL: 9.069352204310357\n",
      "\t Val. Loss: 1.9244128912687302 |  Val. PPL: 6.851125129505524\n",
      "Epoch: 06 | Time: 0m 36s\n",
      "\tTrain Loss: 2.087777865090559 | Train PPL: 8.066969338486139\n",
      "\t Val. Loss: 1.8676335364580154 |  Val. PPL: 6.472960235801158\n",
      "Epoch: 07 | Time: 0m 34s\n",
      "\tTrain Loss: 2.000380798583514 | Train PPL: 7.3918703768291385\n",
      "\t Val. Loss: 1.8381201177835464 |  Val. PPL: 6.284712629604244\n",
      "Epoch: 08 | Time: 0m 34s\n",
      "\tTrain Loss: 1.9234235958905996 | Train PPL: 6.844350694591\n",
      "\t Val. Loss: 1.8006822019815445 |  Val. PPL: 6.053775953972577\n",
      "Epoch: 09 | Time: 0m 35s\n",
      "\tTrain Loss: 1.860280640324832 | Train PPL: 6.425539783990485\n",
      "\t Val. Loss: 1.7873083800077438 |  Val. PPL: 5.973352813176069\n",
      "Epoch: 10 | Time: 0m 34s\n",
      "\tTrain Loss: 1.8021107507697287 | Train PPL: 6.06243024834466\n",
      "\t Val. Loss: 1.756932184100151 |  Val. PPL: 5.794633230459425\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 0.1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'conv_seq2seq.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss} | Train PPL: {math.exp(train_loss)}')\n",
    "    print(f'\\t Val. Loss: {valid_loss} |  Val. PPL: {math.exp(valid_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:05:59.363082Z",
     "start_time": "2021-01-27T16:05:59.057882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 1.836 | Test PPL:   6.269 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('conv_seq2seq.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction \n",
    "    - 1. One important thing to notice here is that the input to the decoder while prediction is different than that of training\n",
    "    - 2. While training we feed the entire target sentence and we generate the entire output sentence. \n",
    "    - 3. While testing this is not possible. So we start with a <sos> and predict the next letter. That becomes the input for the next letter. So we predict in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:06:03.884016Z",
     "start_time": "2021-01-27T16:06:03.856130Z"
    }
   },
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "\n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_conved, encoder_combined = model.encoder(src_tensor)\n",
    "\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, encoder_conved, encoder_combined)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:06:08.282256Z",
     "start_time": "2021-01-27T16:06:08.263663Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "        \n",
    "    attention = attention.squeeze(0).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=15)\n",
    "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
    "                       rotation=45)\n",
    "    ax.set_yticklabels(['']+translation)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:06:12.765136Z",
     "start_time": "2021-01-27T16:06:12.759900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['ein', 'kleines', 'm√§dchen', 'klettert', 'in', 'ein', 'spielhaus', 'aus', 'holz', '.']\n",
      "trg = ['a', 'little', 'girl', 'climbing', 'into', 'a', 'wooden', 'playhouse', '.']\n"
     ]
    }
   ],
   "source": [
    "example_idx = 2\n",
    "\n",
    "src = vars(train_data.examples[example_idx])['src']\n",
    "trg = vars(train_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:06:17.582774Z",
     "start_time": "2021-01-27T16:06:17.512219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted trg = ['a', 'little', 'girl', 'climbing', 'in', 'a', 'playhouse', 'out', 'of', 'wood', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:07:05.020039Z",
     "start_time": "2021-01-27T16:07:04.802125Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-29ba19def7ed>:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'],\n",
      "<ipython-input-38-29ba19def7ed>:13: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(['']+translation)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
